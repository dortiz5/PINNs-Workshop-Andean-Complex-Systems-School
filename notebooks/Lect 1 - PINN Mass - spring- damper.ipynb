{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCgpgtSqTbvP"
   },
   "source": [
    "# PINNs para el sistema masa‚Äìresorte‚Äìamortiguador\n",
    "\n",
    "**Autor:** David Ortiz ‚Äî 2025\n",
    "\n",
    "Accede al trabajo fundacional de las PINNs [aqu√≠](https://www.sciencedirect.com/science/article/pii/S0021999118307125).\n",
    "\n",
    "Este tutorial est√° inspirado en [este](https://github.com/benmoseley/harmonic-oscillator-pinn-workshop) workshop, elaborado por Ben Moseley\n",
    "\n",
    "### Introducci√≥n\n",
    "Las Redes Neuronales Informadas por la F√≠sica (PINNs) incorporan ecuaciones gobernantes en la funci√≥n de p√©rdida mediante diferenciaci√≥n autom√°tica, reduciendo la dependencia de grandes vol√∫menes de datos y mejorando la interpretabilidad. En esta actividad aplicaremos PINNs al modelo lineal cl√°sico **masa‚Äìresorte‚Äìamortiguador**, para el cual existe soluci√≥n anal√≠tica, lo que permitir√° verificar cuantitativamente el desempe√±o del enfoque.\n",
    "\n",
    "### Objetivos de de aprendizaje\n",
    "- Implementar una PINN desde cero en PyTorch para resolver el sistema masa-resorte-amortiguador siguiendo las 6 etapas vistas\n",
    "- Comprender los diferentes componentes de la funci√≥n de p√©rdida de una PINN (residuo de la EDO, condiciones iniciales y de frontera\n",
    "- Entender en mayor detalle c√≥mo se entrenan las PINNs y c√≥mo usar diferenciaci√≥n autom√°tica para calcular derivadas de redes neuronales\n",
    "\n",
    "### Resumen de la Actividad\n",
    "Construir una PINN para el sistema masa-resorte-amortiguador, basandonos en los 6 pasos vistos en el taller te√≥rico:\n",
    "\n",
    "<img src=\"/home/ihealth/Dropbox/Trabajo/Presentaciones/2023-2025/20251112 Escuela andina sistemas complejos/uta-escuela-sistemas-complejos/data/figures/pinns_new_scheme.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "- (1) formular el modelo matem√°tico y su soluci√≥n anal√≠tica; \n",
    "- (2) definir el dominio (temporal); \n",
    "- (3) implementar una ANN como surrogate de $x(t)$;\n",
    "- (4) Diferenciaci√≥n autom√°tica\n",
    "- (5) dise√±ar la loss con t√©rminos f√≠sicos (residuo de la EDO e inicio/condici√≥n inicial) usando autograd; \n",
    "- (6) fijar un optimizador (Adam);\n",
    "\n",
    "Finalmete, ejecutar el ciclo de entrenamiento y prueba, comparando contra la soluci√≥n exacta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importamos funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLwL3oIbTbvS",
    "outputId": "d732db5c-47d7-4528-8888-d66be1381707"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Import Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "# Import the time module to time our training process\n",
    "import time\n",
    "# Ignore Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Actualizaci√≥n de los par√°metros de Matplotlib\n",
    "gray = '#5c5c5c' #'#5c5c5c' '000'\n",
    "mlp.rcParams.update(\n",
    "    {\n",
    "        \"image.cmap\" : 'viridis', # plasma, inferno, magma, cividis\n",
    "        \"text.color\" : gray,\n",
    "        \"xtick.color\" :gray,\n",
    "        \"ytick.color\" :gray,\n",
    "        \"axes.labelcolor\" : gray,\n",
    "        \"axes.edgecolor\" :gray,\n",
    "        \"axes.spines.right\" : False,\n",
    "        \"axes.spines.top\" : False,\n",
    "        \"axes.formatter.use_mathtext\": True,\n",
    "        \"axes.unicode_minus\": False,\n",
    "\n",
    "        'font.size' : 15,\n",
    "        'interactive': False,\n",
    "        \"font.family\": 'sans-serif',\n",
    "        \"legend.loc\" : 'best',\n",
    "        'text.usetex': False,\n",
    "        'mathtext.fontset': 'stix',\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Util function to calculate the relative l2 error\n",
    "def relative_l2_error(u_num, u_ref):\n",
    "    # Calculate the L2 norm of the difference\n",
    "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
    "\n",
    "    # Calculate the L2 norm of the reference\n",
    "    l2_ref = torch.norm(u_ref, p=2)\n",
    "\n",
    "    # Calculate L2 relative error\n",
    "    relative_l2 = l2_diff / l2_ref\n",
    "    return relative_l2\n",
    "\n",
    "# Util function to plot the solutions\n",
    "def plot_comparison(time, theta_true, theta_pred, loss):\n",
    "\n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    t_np = time.detach().cpu().data.numpy()\n",
    "    theta_true_np = theta_true.detach().cpu().data.numpy()\n",
    "    theta_pred_np = theta_pred.detach().cpu().data.numpy()\n",
    "\n",
    "\n",
    "    # Create a figure with 2 subplots\n",
    "    _, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot the true and predicted values\n",
    "    axs[0].plot(t_np, theta_true_np, label = r'$\\theta(t)$ (numerical solution)')\n",
    "    axs[0].plot(t_np, theta_pred_np, label = r'$\\theta_{pred}(t)$ (predicted solution) ')\n",
    "    axs[0].set_title('Angular displacement Numerical Vs. Predicted')\n",
    "    axs[0].set_xlabel(r'Time $(s)$')\n",
    "    axs[0].set_ylabel('Amplitude')\n",
    "    axs[0].set_ylim(-1,1.3)\n",
    "    axs[0].legend(loc='best', frameon=False)\n",
    "\n",
    "\n",
    "    # Plot the difference between the predicted and true values\n",
    "    difference = np.abs(theta_true_np.reshape(-1,1) - theta_pred_np.reshape(-1,1))\n",
    "    axs[1].plot(t_np, difference)\n",
    "    axs[1].set_title('Absolute Difference')\n",
    "    axs[1].set_xlabel(r'Time $(s)$')\n",
    "    axs[1].set_ylabel(r'$|\\theta(t) - \\theta_{pred}(t)|$')\n",
    "    # Display the plot\n",
    "    plt.legend(loc='best', frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the loss values recorded during training\n",
    "    # Create a figure with 1 subplots\n",
    "    _, axs = plt.subplots(1, 1, figsize=(6, 3))\n",
    "    axs.plot(loss)\n",
    "    axs.set_xlabel('Iteration')\n",
    "    axs.set_ylabel('Loss')\n",
    "    axs.set_yscale('log')\n",
    "    axs.set_xscale('log')\n",
    "    axs.set_title('Training Progress')\n",
    "    axs.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modelo matem√°tico del sistema masa‚Äìresorte‚Äìamortiguador\n",
    "\n",
    "El sistema masa‚Äìresorte‚Äìamortiguador describe el movimiento de una masa $m$ sujeta a un resorte de constante el√°stica $k$ y un elemento disipador o amortiguador con coeficiente $c$. Su ecuaci√≥n de movimiento, en ausencia de fuerzas externas, est√° dada por:\n",
    "\n",
    "\\begin{equation*}\n",
    "m\\,\\ddot{x}(t) + c\\,\\dot{x}(t) + k\\,x(t) = 0\n",
    "\\end{equation*}\n",
    "\n",
    "donde:\n",
    "- $x(t)$ es el desplazamiento de la masa respecto a su posici√≥n de equilibrio,  \n",
    "- $\\dot{x}(t)$ es la velocidad,  \n",
    "- $\\ddot{x}(t)$ es la aceleraci√≥n.\n",
    "\n",
    "Dividiendo entre $m$, se obtiene la forma normalizada:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ddot{x}(t) + 2\\zeta\\omega_n\\,\\dot{x}(t) + \\omega_n^2\\,x(t) = 0\n",
    "\\end{equation*}\n",
    "\n",
    "con:\n",
    "\\begin{equation*}\n",
    "\\omega_n = \\sqrt{\\frac{k}{m}} \\quad \\text{(frecuencia natural no amortiguada)}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\zeta = \\frac{c}{2\\sqrt{km}} \\quad \\text{(raz√≥n de amortiguamiento adimensional)}\n",
    "\\end{equation*}\n",
    "\n",
    "Definimos los siguientes par√°metros con los que vamos a trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dominio temporal\n",
    "T = 5.0        # tiempo total de simulaci√≥n\n",
    "x0 = 1.0       # Posici√≥n inicial \n",
    "v0 = 0.0       # velocidad incial\n",
    "wn = 5.0       # Frecuencia natural\n",
    "zeta = 0.2     # raz√≥n de amortiguamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Definici√≥n del dominio (temporal) \n",
    "\n",
    "En esta etapa se define el dominio sobre el cual la PINN ser√° entrenada. El tiempo $t$ se discretiza en un conjunto de puntos uniformemente espaciados dentro del intervalo $[0, T]$. Estos puntos se usar√°n como entradas para la red neuronal durante el entrenamiento, mientras que una malla m√°s densa se emplear√° para evaluar la soluci√≥n exacta y comparar el desempe√±o de la PINN.  \n",
    "\n",
    "> **üí° REMARK!:**  \n",
    "> Cada punto $t_i \\in [0,T],\\ i = 1, \\dots, N_{sample}$ se conoce como **punto de colocaci√≥n**, \n",
    "> y su nombre proviene de la similitud de las PINNs con los m√©todos de colocaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_dominio_temporal(T, N_train=101, N_eval=1000):\n",
    "    \"\"\"Crea el dominio temporal para la PINN.\"\"\"\n",
    "    t_train = torch.linspace(0, T, N_train, \n",
    "                             device=device, \n",
    "                             requires_grad=True).view(-1, 1)  # entrenamiento\n",
    "    t_eval = torch.linspace(0, T, N_eval, \n",
    "                             device=device, \n",
    "                             requires_grad=True).view(-1, 1)  # evaluaci√≥n\n",
    "    return t_train, t_eval # dominio de evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Red neuronal como aproximador de la soluci√≥n\n",
    "\n",
    "En esta etapa se define una red neuronal que aproxima la soluci√≥n de la ecuaci√≥n diferencial mediante un modelo param√©trico. La funci√≥n desconocida $x(t)$ se reemplaza por una red neuronal dependiente de un conjunto de par√°metros $\\Theta$:\n",
    "\n",
    "$$\n",
    "x_{PINN}(t; \\Theta) \\approx x_{real}(t).\n",
    "$$\n",
    "\n",
    "Los par√°metros de la red est√°n dados por los pesos y sesgos de cada capa, agrupados como:\n",
    "\n",
    "$$\n",
    "\\Theta = \\{ W_i, b_i \\}_{i=1}^{L},\n",
    "$$\n",
    "\n",
    "donde $W_i$ y $b_i$ representan respectivamente los pesos y sesgos de la capa $i$, y $L$ es el n√∫mero total de capas de la red. Estos par√°metros ser√°n ajustados durante el entrenamiento para minimizar el error entre la ecuaci√≥n f√≠sica y la salida de la red.\n",
    "\n",
    "> **üí° REMARK 1:**  \n",
    "> Para este ejercicio utilizaremos un perceptr√≥n multicapa, con funciones de \n",
    "> activaci√≥n tahn, y una inicializaci√≥n de pesos con esquema de Glorot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network class with user defined layers and neurons\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hlayers = [1, 10, 10, 1]):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Diferenciaci√≥n autom√°tica en PyTorch \n",
    "\n",
    "Antes de continuar ser√° √∫til aprender a calcular las derivadas de una red neuronal. Para esto utilizaremos la diferenciaci√≥n autom√°tica (*autodiff*), que es una t√©cnica para calcular gradientes de funciones de forma eficiente y precisa mediante el uso de la regla de la cadena. PyTorch registra la forma en que operamos las variables en un gr√°fico computacional din√°mico, que luego le permite calcular derivadas autom√°ticamente al realizar una \"propagaci√≥n hacia atr√°s\" (*backpropagation*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to calculate tensor gradients with autodiff\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\"Computes the partial derivative of an output with respect\n",
    "    to an input.\n",
    "    Args:\n",
    "        outputs: (N, 1) tensor\n",
    "        inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                        grad_outputs=torch.ones_like(outputs),\n",
    "                        create_graph=True,\n",
    "                        )[0]\n",
    "\n",
    "\n",
    "# Definir tensor de entrada. Si queremos derivar c/r a x necesitamos inicializar con requires_grad=True\n",
    "x = torch.tensor([1.0, 2.0, 3.0], device=device, requires_grad=True).view(-1,1).float() # (N,1)\n",
    "\n",
    "# Calcular operaci√≥n que dependen de x\n",
    "y = x**2 # (N,1)  \n",
    "\n",
    "# Calcular derivadas c/r a x \n",
    "# grad es un wrapper de torch.autograd\n",
    "dy_dx = grad(y, x) \n",
    "\n",
    "# Calcular derivadas de orden superior\n",
    "d2y_dx2 = grad(dy_dx, x)  \n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y = x^2:\", y)\n",
    "print(\"dy/dx:\", dy_dx)\n",
    "print(\"d^2y/dx^2:\", d2y_dx2)  \n",
    "\n",
    "# Esto tambi√©n funciona para redes neuronales\n",
    "# test_ANN = NeuralNetwork()\n",
    "\n",
    "# NNx = test_ANN(x)\n",
    "# dNNx_dx = grad(NNx, x)\n",
    "# print(\"NNx: \", dNNx_dx)\n",
    "# print(\"dNNx/dx:\", dNNx_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Funci√≥n de p√©rdida informada por la f√≠sica del problema\n",
    "\n",
    "Para entrenar la PINN, utilizamos el modelo del sistema masa‚Äìresorte‚Äìamortiguador en su forma normalizada, donde la frecuencia natural $\\omega_n$ y la raz√≥n de amortiguamiento $\\zeta$ caracterizan la din√°mica:\n",
    "\n",
    "$$\n",
    "\\ddot{x}(t) + 2\\zeta\\omega_n\\,\\dot{x}(t) + \\omega_n^2 x(t) = 0\n",
    "$$\n",
    "\n",
    "Reemplazamos la soluci√≥n desconocida por la salida de la red neuronal $x_{PINN}(t;\\Theta)$ y definimos el residuo f√≠sico y las condiciones iniciales:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{ode}(t;\\,x_{PINN}) := &\\;\n",
    "\\frac{d^2x_{PINN}(t; \\Theta)}{dt^2}\n",
    "+ 2\\zeta\\omega_n \\frac{dx_{PINN}(t; \\Theta)}{dt}\n",
    "+ \\omega_n^2 x_{PINN}(t; \\Theta) = 0, \\\\\n",
    "g_{ic}(0;\\,x_{PINN}) :=&\\;\n",
    "x_{PINN}(0;\\Theta) = x_0, \\\\\n",
    "h_{ic}(0;\\,x_{PINN}) :=&\\;\n",
    "\\frac{dx_{PINN}(0;\\Theta)}{dt} = v_0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "La funci√≥n de p√©rdida total que se optimiza es:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\Theta) := &\n",
    "\\frac{\\lambda_1}{N}\\sum_i \\left( f_{ode}(t_i;\\,x_{PINN}) \\right)^2\n",
    "+ \\lambda_2 \\left( g_{ic}(0;\\,x_{PINN}) - x_0 \\right)^2\n",
    "+ \\lambda_3 \\left( h_{ic}(0;\\,x_{PINN}) - v_0 \\right)^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "El entrenamiento busca minimizar esta funci√≥n:\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta} \\mathcal{L}(\\Theta) \\rightarrow 0\n",
    "$$\n",
    "\n",
    "utilizando diferenciaci√≥n autom√°tica (`torch.autograd`) para calcular las derivadas de la red con respecto al tiempo, necesarias para evaluar el residuo f√≠sico del modelo.\n",
    "\n",
    "> **üí° REMARK:**  \n",
    "> Cuando no incluimos la funci√≥n de p√©rdida relacionada con los datos, estamos \n",
    "> empleando un esquema independiente de datos (*data-free*); cuando incluimos \n",
    "> los datos, estamos empleando un esquema impulsado por datos (*data-driven*).\n",
    "\n",
    "\n",
    "> **üí° REMARK:**  \n",
    "> En este esquema, las condiciones iniciales y de frontera se cumplen d√©bilmente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# derivatives of the ANN\n",
    "def PINNLoss(PINN, t_phys, wn, zeta, x0 = 1, v0 = 0,\n",
    "             lambda1 = 1, lambda2 = 1, lambda3 = 1):\n",
    "\n",
    "    t0 = torch.tensor(0., device=device, requires_grad=True).view(-1,1)\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    x_pinn_t = PINN(t_phys)\n",
    "    x_pinn_dt = grad(x_pinn_t, t_phys)\n",
    "    x_pinn_ddt = grad(x_pinn_dt, t_phys)\n",
    "    \n",
    "    f_ode = x_pinn_ddt + 2 * zeta * wn * x_pinn_dt + wn**2 * x_pinn_t\n",
    "    ODE_loss = lambda1 * MSE_func(f_ode, torch.zeros_like(f_ode)) \n",
    "    \n",
    "    g_ic = PINN(t0)\n",
    "    IC_loss = lambda2 * MSE_func(g_ic, torch.ones_like(g_ic)*x0)\n",
    "    \n",
    "    h_bc = grad(PINN(t0),t0)\n",
    "    BC_loss = lambda3 * MSE_func(h_bc, torch.zeros_like(h_bc)*v0)\n",
    "    \n",
    "    return ODE_loss + IC_loss + BC_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Definici√≥n del optimizador\n",
    "\n",
    "Una vez definida la funci√≥n de p√©rdida, se selecciona un optimizador para ajustar los par√°metros $\\Theta = \\{W_i, b_i\\}$ de la red neuronal. El objetivo del optimizador es minimizar la funci√≥n de p√©rdida informada por la f√≠sica, es decir:\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta} \\mathcal{L}(\\Theta)\n",
    "$$\n",
    "\n",
    "En esta etapa se emplea el optimizador **ADAM**, un m√©todo de descenso de gradiente adaptativo que ajusta din√°micamente la tasa de aprendizaje para cada par√°metro. ADAM combina las ventajas del momento cl√°sico y del escalado adaptativo de gradientes, lo que lo hace especialmente adecuado para el entrenamiento de PINNs.\n",
    "\n",
    "> **üí° REMARK:**  \n",
    "> Cuando no incluimos la funci√≥n de p√©rdida relacionada con los datos, estamos \n",
    "> empleando un esquema independiente de datos (*data-free*); cuando incluimos \n",
    "> los datos, estamos empleando un esquema impulsado por datos (*data-driven*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_opimizer(pinn, lr = 0.01):\n",
    "\n",
    "    # Define an optimizer (Adam) for training the network\n",
    "    return optim.Adam(pinn.parameters(), lr=lr,\n",
    "                        betas= (0.99,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ciclo de entrenamiento\n",
    "\n",
    "En esta etapa se ejecuta el proceso iterativo mediante el cual la PINN ajusta sus par√°metros $\\Theta = \\{W_i, b_i\\}$ para minimizar la funci√≥n de p√©rdida definida anteriormente. Durante cada √©poca (*epoch*), el modelo eval√∫a el residuo de la ecuaci√≥n diferencial y las condiciones iniciales sobre los puntos de colocaci√≥n $t_i \\in [0, T]$, actualizando los par√°metros seg√∫n el gradiente de la p√©rdida:\n",
    "\n",
    "$$\n",
    "\\Theta \\leftarrow \\Theta - \\eta \\, \\nabla_\\Theta \\mathcal{L}(\\Theta)\n",
    "$$\n",
    "\n",
    "donde $\\eta$ es la tasa de aprendizaje. El ciclo contin√∫a hasta que la p√©rdida alcanza un valor suficientemente peque√±o o deja de mejorar significativamente. Finalmente, la soluci√≥n obtenida $x_{PINN}(t; \\Theta)$ se compara con la soluci√≥n exacta para evaluar la calidad del entrenamiento y la capacidad del modelo de reproducir la din√°mica del sistema masa‚Äìresorte‚Äìamortiguador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n se presenta el c√≥digo completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# ETAPA 1: DEFINICI√ìN DE LOS PAR√ÅMETROS (MODELO F√çSICO)\n",
    "#===============================================================================\n",
    "# Dominio temporal\n",
    "T = 5.0        # tiempo total de simulaci√≥n\n",
    "x0 = 1.0       # Posici√≥n inicial \n",
    "v0 = 0.0       # velocidad incial\n",
    "wn = 5.0  # Frecuencia natural\n",
    "zeta = 0.2     # raz√≥n de amortiguamiento\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 2: DEFINICI√ìN DEL DOMINIO \n",
    "#===============================================================================\n",
    "# Creamos los tensores de tiempo para el entrenamiento y la evaluaci√≥n\n",
    "t_train, t_eval = crear_dominio_temporal(T)\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 3: CREACI√ìN DE LA RED NEURONAL SURROGANTE \n",
    "#===============================================================================\n",
    "# Creamos la ANN\n",
    "torch.manual_seed(123)\n",
    "hidden_layers = [1, 30, 30, 30, 1]# Par√°metros de la \n",
    "\n",
    "# Create an instance of the neural network\n",
    "x_pinn = NeuralNetwork(hidden_layers).to(device)\n",
    "nparams = sum(p.numel() for p in x_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "#==========================================================================\n",
    "# ETAPA 4 Y 5: DEFINICI√ìN DE LA FUNCI√ìN DE COSTO BASADA EN AUTOGRAD\n",
    "#==========================================================================\n",
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# derivatives of the ANN\n",
    "def PINNLoss(PINN, t_phys, wn, zeta, x0 = 1, v0 = 0, \n",
    "             w1 = 1, w2 = 1, w3 = 1):\n",
    "\n",
    "    t0 = torch.tensor(0., device=device, requires_grad=True).view(-1,1)\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    x_pinn_t = PINN(t_phys)\n",
    "    x_pinn_dt = grad(x_pinn_t, t_phys)\n",
    "    x_pinn_ddt = grad(x_pinn_dt, t_phys)\n",
    "    \n",
    "    f_ode = x_pinn_ddt + 2 * zeta * wn * x_pinn_dt + wn**2 * x_pinn_t\n",
    "    ODE_loss = w1 * MSE_func(f_ode, torch.zeros_like(f_ode)) \n",
    "    \n",
    "    g_ic = PINN(t0)\n",
    "    IC_loss = w2 * MSE_func(g_ic, torch.ones_like(g_ic)*x0)\n",
    "    \n",
    "    h_bc = grad(PINN(t0),t0)\n",
    "    BC_loss = w3 * MSE_func(h_bc, torch.zeros_like(h_bc)*v0)\n",
    "    \n",
    "    return ODE_loss + IC_loss + BC_loss \n",
    "\n",
    "#==========================================================================\n",
    "# ETAPA 6: DEFINICI√ìN DEl OPTIMIZADOR\n",
    "#==========================================================================\n",
    "lr = 0.01\n",
    "optimizer = pinn_opimizer(x_pinn, lr)\n",
    "\n",
    "#==========================================================================\n",
    "# CICLO DE ENTRENAMIENTO\n",
    "#==========================================================================\n",
    "training_iter = 20000\n",
    "\n",
    "# Initialize a list to store the loss values\n",
    "loss_values_pinn = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = PINNLoss(x_pinn, t_train, wn, zeta)\n",
    "\n",
    "    # Append the current loss value to the list\n",
    "    loss_values_pinn.append(loss.item())\n",
    "\n",
    "    if i % 500 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "\n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validaci√≥n\n",
    "\n",
    "El comportamiento del sistema depende de $\\zeta$:\n",
    "\n",
    "- **Subamortiguado** ($0 < \\zeta < 1$): el sistema oscila con frecuencia amortiguada  \n",
    "  \\begin{equation*}\n",
    "  \\omega_d = \\omega_n \\sqrt{1 - \\zeta^2}\n",
    "  \\end{equation*}\n",
    "\n",
    "  la soluci√≥n puede escribirse en t√©rminos de las condiciones iniciales $x(0)=x_0$ y $\\dot{x}(0)=v_0$:\n",
    "  \\begin{equation*}\n",
    "  x(t) = e^{-\\zeta \\omega_n t}\n",
    "  \\left[\n",
    "  x_0 \\cos(\\omega_d t) +\n",
    "  \\frac{v_0 + \\zeta \\omega_n x_0}{\\omega_d}\n",
    "  \\sin(\\omega_d t)\n",
    "  \\right]\n",
    "  \\end{equation*}\n",
    "\n",
    "- **Cr√≠ticamente amortiguado** ($\\zeta = 1$): no hay oscilaciones, el sistema retorna a equilibrio lo m√°s r√°pido posible sin sobrepasarlo:\n",
    "  \\begin{equation*}\n",
    "  x(t) = (x_0 + (v_0 + \\omega_n x_0)t)\\, e^{-\\omega_n t}\n",
    "  \\end{equation*}\n",
    "\n",
    "- **Sobreamortiguado** ($\\zeta > 1$): el sistema retorna al equilibrio sin oscilar, pero m√°s lentamente:\n",
    "  \\begin{equation*}\n",
    "  x(t) =\n",
    "  \\frac{v_0 - r_2 x_0}{r_1 - r_2} e^{r_1 t} +\n",
    "  \\frac{r_1 x_0 - v_0}{r_1 - r_2} e^{r_2 t}\n",
    "  \\end{equation*}\n",
    "\n",
    "  con:\n",
    "  \\begin{equation*}\n",
    "  r_{1,2} = -\\omega_n\\left( \\zeta \\pm \\sqrt{\\zeta^2 - 1} \\right)\n",
    "  \\end{equation*}\n",
    "\n",
    "\n",
    "Esta expresi√≥n servir√° como referencia para validar la soluci√≥n obtenida mediante la red neuronal informada por la f√≠sica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masa_resorte_general(t, x0, v0, omega_n, zeta):\n",
    "    \"\"\"\n",
    "    Soluci√≥n exacta x(t) del sistema masa-resorte-amortiguador:\n",
    "        x'' + 2*zeta*omega_n*x' + omega_n^2*x = 0\n",
    "    Incluye los tres reg√≠menes (sub, cr√≠tico y sobreamortiguado).\n",
    "    \"\"\"\n",
    "    t = np.array(t, dtype=float)\n",
    "\n",
    "    if 0 < zeta < 1:  # Subamortiguado\n",
    "        omega_d = omega_n * np.sqrt(1 - zeta**2)\n",
    "        x = np.exp(-zeta * omega_n * t) * (\n",
    "            x0 * np.cos(omega_d * t) +\n",
    "            (v0 + zeta * omega_n * x0) / omega_d * np.sin(omega_d * t)\n",
    "        )\n",
    "\n",
    "    elif np.isclose(zeta, 1.0):  # Cr√≠ticamente amortiguado\n",
    "        x = np.exp(-omega_n * t) * (\n",
    "            x0 * (1 + omega_n * t) + v0 * t\n",
    "        )\n",
    "\n",
    "    elif zeta > 1:  # Sobreamortiguado\n",
    "        r1 = -omega_n * (zeta - np.sqrt(zeta**2 - 1))\n",
    "        r2 = -omega_n * (zeta + np.sqrt(zeta**2 - 1))\n",
    "        x = (\n",
    "            ((v0 - r2 * x0) / (r1 - r2)) * np.exp(r1 * t) +\n",
    "            ((r1 * x0 - v0) / (r1 - r2)) * np.exp(r2 * t)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"zeta debe ser mayor que 0.\")\n",
    "\n",
    "    return torch.tensor(x, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "# -------------------------------------------\n",
    "# solucion exacta\n",
    "t_eval_np = t_eval.detach().cpu().numpy().ravel()\n",
    "x = masa_resorte_general(t_eval_np, x0, v0, wn, zeta)\n",
    "\n",
    "# predicci√≥n de la PINN\n",
    "x_pred_pinn = x_pinn(t_eval)\n",
    "\n",
    "print(f'Relative error: {relative_l2_error(x_pred_pinn, x)}')\n",
    "\n",
    "plot_comparison(t_eval, x, x_pred_pinn, loss_values_pinn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicios**:\n",
    "1. Ajusta los valores de los par√°metros del sistema din√°mico y analiza su impacto en la convergencia de la PINN.\n",
    "2. Ajusta los valores de los par√°metros `lambdas` en la funci√≥n de p√©rdida para ambas redes y analiza su impacto.\n",
    "2. Modifica la tasa de aprendizaje (`learning_rate`) del optimizador y el n√∫mero de iteraciones de entrenamiento, y eval√∫a el efecto en el desempe√±o.\n",
    "3. Cambia el n√∫mero de capas ocultas (`hidden_layers`), neuronas y funciones de activaci√≥n de la red neuronal, y observa el impacto en los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicio: Sistema de Lorenz con PINNs**\n",
    "\n",
    "### **Contexto**\n",
    "\n",
    "El sistema de Lorenz es un conjunto de tres ecuaciones diferenciales ordinarias acopladas que describe un sistema ca√≥tico. Fue introducido por Edward Lorenz en 1963 como un modelo simplificado de convecci√≥n atmosf√©rica y se ha convertido en uno de los ejemplos m√°s ic√≥nicos de comportamiento ca√≥tico en sistemas din√°micos.\n",
    "\n",
    "### **Modelo Matem√°tico**\n",
    "\n",
    "El sistema de Lorenz est√° definido por las siguientes ecuaciones:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} &= \\sigma(y - x) \\\\\n",
    "\\frac{dy}{dt} &= x(\\rho - z) - y \\\\\n",
    "\\frac{dz}{dt} &= xy - \\beta z\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $x(t)$, $y(t)$, $z(t)$ son las variables de estado del sistema\n",
    "- $\\sigma$ (sigma) es el n√∫mero de Prandtl, que representa la relaci√≥n entre viscosidad y conductividad t√©rmica\n",
    "- $\\rho$ (rho) es el n√∫mero de Rayleigh, relacionado con la diferencia de temperatura\n",
    "- $\\beta$ (beta) es un par√°metro geom√©trico relacionado con las dimensiones f√≠sicas del sistema\n",
    "\n",
    "### **Par√°metros y Condiciones Iniciales**\n",
    "\n",
    "Para este ejercicio, utiliza los **par√°metros cl√°sicos de Lorenz** que producen comportamiento ca√≥tico:\n",
    "\n",
    "$$\n",
    "\\sigma = 10.0, \\quad \\rho = 28.0, \\quad \\beta = \\frac{8}{3}\n",
    "$$\n",
    "\n",
    "**Condiciones iniciales:**\n",
    "\n",
    "$$\n",
    "x(0) = 1.0, \\quad y(0) = 1.0, \\quad z(0) = 1.0\n",
    "$$\n",
    "\n",
    "**Dominio temporal:** $t \\in [0, 20]$\n",
    "\n",
    "### **Objetivo**\n",
    "\n",
    "Implementar una **Physics-Informed Neural Network (PINN)** siguiendo las 6 etapas del flujo de trabajo para:\n",
    "\n",
    "1. Aproximar las soluciones $x(t)$, $y(t)$, $z(t)$ del sistema de Lorenz\n",
    "2. Comparar los resultados con la soluci√≥n num√©rica obtenida mediante el m√©todo de Runge-Kutta\n",
    "3. Visualizar el famoso **atractor de Lorenz** en el espacio de fases $(x, y, z)$\n",
    "4. Analizar la capacidad de la PINN para capturar la din√°mica ca√≥tica del sistema\n",
    "\n",
    "> **üí° REMARK:**  \n",
    "> Para este ejercicio debes utilizar un enfoque data-driven. Es decir, en la \n",
    "> funci√≥n de costo debes incluir los datos simulados. Si tienes dudas de como > se hace, puedes ver la soluci√≥n del ejercicio en la carpeta \"solutions\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO MODIFIQUES EL SIGUIENTE C√ìDIGO\n",
    "\n",
    "Sin embargo, analiza cual es la diferencia con la red neuronal planteada en el ejercicio anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "#===============================================================================\n",
    "# ARQUITECTURA MODIFICADA: NO TOCAR\n",
    "#===============================================================================\n",
    "\n",
    "class Sin(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hlayers, fourier_dim=None, sigma=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hlayers (list): lista con el n√∫mero de neuronas en cada capa.\n",
    "            fourier_dim (int): dimensi√≥n de las Fourier features (opcional).\n",
    "            sigma (float): escala de las frecuencias aleatorias.\n",
    "        \"\"\"\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.fourier_dim = fourier_dim\n",
    "        self.sigma = sigma\n",
    "\n",
    "        if self.fourier_dim is not None:\n",
    "            # Inicializamos matriz B ~ N(0, sigma^2)\n",
    "            input_dim = hlayers[0]\n",
    "            B = torch.randn((fourier_dim, input_dim)) * sigma\n",
    "            self.register_buffer(\"B\", B)  # se guarda como parte del modelo pero no se entrena\n",
    "            # actualizamos la entrada de la red: ahora 2*fourier_dim\n",
    "            hlayers = [2 * fourier_dim] + hlayers[1:]\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(Sin())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params()\n",
    "\n",
    "    def fourier_features(self, x):\n",
    "        \"\"\"Mapeo de Fourier features\"\"\"\n",
    "        # x shape: [batch, input_dim]\n",
    "        x_proj = 2 * torch.pi * x @ self.B.T  # [batch, fourier_dim]\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.fourier_dim is not None:\n",
    "            x = self.fourier_features(x)\n",
    "        return self.layers(x)\n",
    "    \n",
    "#===============================================================================\n",
    "# SOLUCI√ìN NUM√âRICA. NO TOCAR\n",
    "#===============================================================================\n",
    "\n",
    "def numerical_sol_lorenz(t_eval, sigma = 10, rho = 28, beta = 8.0/3.0,\n",
    "                         x0 = 1, y0 = 1, z0 = 1):\n",
    "        \n",
    "    def lorenz_system(t, state, sigma, rho, beta):\n",
    "        x, y, z = state\n",
    "        dx_dt = sigma * (y - x)\n",
    "        dy_dt = x * (rho - z) - y\n",
    "        dz_dt = x * y - beta * z\n",
    "        return [dx_dt, dy_dt, dz_dt]\n",
    "\n",
    "    # Resolver con m√©todo num√©rico\n",
    "    t_span = (0, T)\n",
    "    t_eval_np = t_eval.detach().cpu().numpy().ravel()\n",
    "    initial_state = [x0, y0, z0]\n",
    "\n",
    "    sol = solve_ivp(\n",
    "        lorenz_system, \n",
    "        t_span, \n",
    "        initial_state,\n",
    "        args=(sigma, rho, beta),\n",
    "        t_eval=t_eval_np,\n",
    "        method='RK45'\n",
    "    )\n",
    "\n",
    "    x_true = torch.tensor(sol.y[0],device=device, dtype=torch.float32).view(-1, 1)\n",
    "    y_true = torch.tensor(sol.y[1],device=device, dtype=torch.float32).view(-1, 1)\n",
    "    z_true = torch.tensor(sol.y[2],device=device, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    return x_true, y_true, z_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESTE C√ìDIGO SE PUEDES MODIFICAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# ETAPA 1: DEFINICI√ìN DE LOS PAR√ÅMETROS (MODELO F√çSICO)\n",
    "#===============================================================================\n",
    "\n",
    "sigma = 10.0\n",
    "rho = 28.0\n",
    "beta = 8.0/3.0\n",
    "\n",
    "x0 = 1.0\n",
    "y0 = 1.0\n",
    "z0 = 1.0\n",
    "\n",
    "# Dominio temporal\n",
    "T = 20.0        # tiempo total de simulaci√≥n\n",
    "N_train = 100   # puntos de colocaci√≥n para entrenamiento\n",
    "N_eval = 2000   # puntos para evaluaci√≥n\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 2: DEFINICI√ìN DEL DOMINIO \n",
    "#===============================================================================\n",
    "t_train, t_eval = crear_dominio_temporal(T, N_train, N_eval)\n",
    "\n",
    "x_train, y_train, z_train = numerical_sol_lorenz(t_train)\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 3: CREACI√ìN DE LA RED NEURONAL SURROGANTE \n",
    "#===============================================================================\n",
    "\n",
    "# Crear la red\n",
    "torch.manual_seed(123)\n",
    "hidden_layers = [1, 50, 50, 50, 3]\n",
    "lorenz_pinn = NeuralNetwork(hidden_layers).to(device)\n",
    "nparams = sum(p.numel() for p in lorenz_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "#==========================================================================\n",
    "# ETAPA 4 Y 5: DEFINICI√ìN DE LA FUNCI√ìN DE COSTO BASADA EN AUTOGRAD\n",
    "#==========================================================================\n",
    "MSE = nn.MSELoss()\n",
    "\n",
    "def LorenzPINNLoss(PINN, t_phys, sigma, rho, beta, \n",
    "                   x0=1.0, y0=1.0, z0=1.0,\n",
    "                   lambda_pde=10.0, lambda_ic=10.0, lambda_data = 100):\n",
    "    \n",
    "    t0 = torch.tensor(0., device=device, requires_grad=True).view(-1, 1)\n",
    "    \n",
    "    # Predicciones de la red\n",
    "    #TODO: escribe c√≥digo aqu√≠\n",
    "    \n",
    "    # Calcular derivadas temporales\n",
    "    #TODO: escribe c√≥digo aqu√≠\n",
    "    \n",
    "    # Residuos de las ecuaciones de Lorenz\n",
    "    #TODO: escribe c√≥digo aqu√≠\n",
    "\n",
    "    \n",
    "    # P√©rdida PDE (residuo de las ecuaciones)\n",
    "    #TODO: escribe c√≥digo aqu√≠\n",
    "\n",
    "    \n",
    "    # Condiciones iniciales\n",
    "    #TODO: escribe c√≥digo aqu√≠\n",
    "\n",
    "    # P√©rdida de los datos\n",
    "    #TODO: escribe c√≥digo aqu√≠\n",
    "    \n",
    "    # P√©rdida total\n",
    "    return lambda_pde * loss_pde + lambda_ic * loss_ic + lambda_data*loss_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#==========================================================================\n",
    "# ETAPA 6: DEFINICI√ìN DEl OPTIMIZADOR\n",
    "#==========================================================================\n",
    "lr = 0.01\n",
    "optimizer = pinn_opimizer(lorenz_pinn, lr)\n",
    "\n",
    "#==========================================================================\n",
    "# CICLO DE ENTRENAMIENTO\n",
    "#==========================================================================\n",
    "training_iter = 30000\n",
    "loss_values = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = LorenzPINNLoss(lorenz_pinn, t_train, sigma, rho, beta)\n",
    "    \n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Iteration {i}: Loss {loss.item():.10f}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Training time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "#==========================================================================\n",
    "# Validaci√≥n\n",
    "#==========================================================================\n",
    "\n",
    "# Primero, obt√©n la soluci√≥n num√©rica completa para graficar\n",
    "x_true, y_true, z_true = numerical_sol_lorenz(t_eval, sigma, rho, beta, x0, y0, z0)\n",
    "\n",
    "# Convertir a numpy para graficar\n",
    "x_true_np = x_true.detach().cpu().numpy().ravel()\n",
    "y_true_np = y_true.detach().cpu().numpy().ravel()\n",
    "z_true_np = z_true.detach().cpu().numpy().ravel()\n",
    "\n",
    "# Predicciones de la PINN\n",
    "lorenz_pinn.eval()\n",
    "with torch.no_grad():\n",
    "    u_pred = lorenz_pinn(t_eval).cpu().numpy()\n",
    "    x_pred = u_pred[:, 0]\n",
    "    y_pred = u_pred[:, 1]\n",
    "    z_pred = u_pred[:, 2]\n",
    "\n",
    "t_plot = t_eval.detach().cpu().numpy().ravel()\n",
    "\n",
    "# Gr√°fico 1: Series temporales\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "axes[0].plot(t_plot, x_true_np, 'g-', label='Soluci√≥n num√©rica', linewidth=2)\n",
    "axes[0].plot(t_plot, x_pred, 'r--', label='PINN', linewidth=1.5)\n",
    "axes[0].set_ylabel('x(t)', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(t_plot, y_true_np, 'g-', label='Soluci√≥n num√©rica', linewidth=2)\n",
    "axes[1].plot(t_plot, y_pred, 'r--', label='PINN', linewidth=1.5)\n",
    "axes[1].set_ylabel('y(t)', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(t_plot, z_true_np, 'g-', label='Soluci√≥n num√©rica', linewidth=2)\n",
    "axes[2].plot(t_plot, z_pred, 'r--', label='PINN', linewidth=1.5)\n",
    "axes[2].set_xlabel('Tiempo (s)', fontsize=12)\n",
    "axes[2].set_ylabel('z(t)', fontsize=12)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fico 2: Atractor de Lorenz (3D)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Soluci√≥n num√©rica\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot(x_true_np, y_true_np, z_true_np, 'g-', linewidth=0.5, alpha=0.7)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('z')\n",
    "ax1.set_title('Atractor de Lorenz - Soluci√≥n Num√©rica')\n",
    "ax1.view_init(elev=20, azim=45)  # Ajustar vista\n",
    "\n",
    "# PINN\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.plot(x_pred, y_pred, z_pred, 'r-', linewidth=0.5, alpha=0.7)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_zlabel('z')\n",
    "ax2.set_title('Atractor de Lorenz - PINN')\n",
    "ax2.view_init(elev=20, azim=45)  # Ajustar vista\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fico 3: P√©rdida durante el entrenamiento\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel('Iteraci√≥n')\n",
    "plt.ylabel('P√©rdida')\n",
    "plt.yscale('log')\n",
    "plt.title('Convergencia del Entrenamiento')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fico 4: Errores relativos\n",
    "\n",
    "# Convertir predicciones a tensores de PyTorch si no lo son\n",
    "x_pred_torch = torch.tensor(x_pred, device=device, dtype=torch.float32).view(-1, 1)\n",
    "y_pred_torch = torch.tensor(y_pred, device=device, dtype=torch.float32).view(-1, 1)\n",
    "z_pred_torch = torch.tensor(z_pred, device=device, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Calcular errores (ahora ambos son tensores de PyTorch)\n",
    "error_x = torch.norm(x_pred_torch - x_true) / torch.norm(x_true)\n",
    "error_y = torch.norm(y_pred_torch - y_true) / torch.norm(y_true)\n",
    "error_z = torch.norm(z_pred_torch - z_true) / torch.norm(z_true)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"M√âTRICAS DE ERROR\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Error L2 relativo en x: {error_x.item():.6f}\")\n",
    "print(f\"Error L2 relativo en y: {error_y.item():.6f}\")\n",
    "print(f\"Error L2 relativo en z: {error_z.item():.6f}\")\n",
    "print(f\"Error L2 promedio: {((error_x + error_y + error_z)/3).item():.6f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
